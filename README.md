# YouTube-Annotations

Кто делал проект? 
Зайцева Виктория, Сахаутдинова Анжелика

Презентация на проект: https://docs.google.com/presentation/d/1Qi5ih6-lygcPj904fNYidiIbD9l8Tw-T6RY2JNK0kro/edit#slide=id.g2079023d5ac_0_55

Ссылка на проект: https://github.com/vizaytseva/YouTube-Annotations

### Наша идея

Основной идеей является проверка и сравнение результатов уже существующих моделей для реферирования текстов на английском языке между собой. 
Для этого мы берем несколько видео на платформе Youtube, выбираем три модели саммаризации текстов, тестируем данные модели, а затем проводим сравнительный анализ всех трех результатов.

### Этапы работы:

Определить гипотезы
Отобрать видео
Выгрузить субтитры с Youtube при помощи Api
Провести предобработку, расставить знаки при помощи torch
Создать корпус текстов
Создать аннотации при помощи трех моделей
Сравнить результаты на сходство при помощи библиотеки difflib
Проверить гипотезы
 
### Какие тексты мы берем?

Мы решили взять 6 видео на платформе YouTube, различных по жанру, длине и формату:
#### Влоги.
Короткое видео   (автосубтитры)
Длинное видео   (автосубтитры)
#### Интервью.
Короткое видео.
Длинное видео.
#### Новостные видео.
Короткое видео.
Длинное видео.

Относительно длины видео нами были выбраны как короткие, так и средней
длины видео: около 5 минут и около 30-40 минут. Это позволит нам изучить, как длина видео влияет на качество исходной саммаризации.

### Почему мы берем эти тексты?

Мы решили взять тексты из разных жанров, которые отличаются между собой степенью спонтанности речи. Это позволит нам изучить, как реферируются разные стили речи.

### Гипотезы

##### Наши основные гипотезы в том, что:

Результат аннотации из видео с автоматическими субтитрами будет хуже, чем результат из видео со встроенными субтитрами
Три модели саммаризации выдают разный результат, несмотря на одинаковый исходный материал

### Какие модели мы проверяем?

Bert (bert-extractive-summarizer)
SBert (SBertSummarizer)
Bart-cnn-12-6 (sshleifer/distilbart-cnn-12-6)

### Результаты моделей, сравнение

Модели некорректно сработали на автоматических субтитрах. В остальных случаях результаты были неплохие. На длинные видео саммаризация на 3-5 предложений делается хуже, чем на короткие. 
Наиболее похожи аннотации у моделей Bert и Bart-cnn (Pipeline) около 80%.

### Заключение: подтвердились ли гипотезы?

Автоматические субтитры самматизируются некорректно 
Улучшенная модель SBart работает быстрее
Pipeline самматизирует точнее всего тексты из встроенных субтитров

### Вопросы для исследования:
Проверить насколько сильно совпадают саммаризации, написанные человеком и с тем, ккак они пишутся автоматически.

